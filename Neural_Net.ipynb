{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "// TODO: Write a introduction part here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "To get started with any ML project, at first we need data, so lets import it. Typically, we would also perform data cleaning and wrangling here, but since our data is already clean and well-structured, we only ensure that the data types are read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Read CSV file without checking for missing values\n",
    "data = []\n",
    "with open(\"data/full_data.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    headers = next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        # Convert relevant columns to numerical values\n",
    "        income = float(row[1])\n",
    "        age = float(row[2])\n",
    "        loan = float(row[3])\n",
    "        target = int(row[4])\n",
    "        # Append the row to the data\n",
    "        data.append([row[0], income, age, loan, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Data into Test & Training Sets\n",
    "\n",
    "In this step, we will split the data into two parts: training and test data. The training data will be used to teach the model, while the test data will help us check how well the model works on new, unseen data. This way, we can ensure that the model is not just memorizing the training data but is actually learning patterns that can be applied in real situations.\n",
    "\n",
    "A model can \"memorize\" the data if trained for too long on a small dataset. This is called overfitting. When overfitting happens, the model performs very well on the training data but struggles with new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(data)\n",
    "\n",
    "# Define the split ratio (80% training, 20% testing)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "\n",
    "# Save the training data to a CSV file\n",
    "with open(\"data/train_data.csv\", \"w\", newline=\"\") as trainfile:\n",
    "    writer = csv.writer(trainfile)\n",
    "    writer.writerow(headers)  # Write the header\n",
    "    writer.writerows(train_data)  # Write the training data\n",
    "\n",
    "\n",
    "# Save the testing data to a CSV file\n",
    "with open(\"data/test_data.csv\", \"w\", newline=\"\") as testfile:\n",
    "    writer = csv.writer(testfile)\n",
    "    writer.writerow(headers)  # Write the header\n",
    "    writer.writerows(test_data)  # Write the testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction and Target Variable Selection\n",
    "\n",
    "Now, that we have divided our data into `test` and `training` sets, lets pick the columns (`income`, `age`, and `loan`) based on what we want to predict the outcome or a last column (`class`).\n",
    "\n",
    "The columns `income`, `age`, and `loan` are selected as features, which are stored in `X_train` and `X_test`. These features will be used as input to train and evaluate the model.\n",
    "\n",
    "Additionally, we select the last column (`class`) as the target variable, which is stored in `y_train` and `y_test`. The target variable represents the outcome or category we want the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target from training data\n",
    "X_train = [[col[1], col[2], col[3]] for col in train_data]  # income, age, loan\n",
    "y_train = [col[4] for col in train_data]  # class (target)\n",
    "\n",
    "# Extract features and target from testing data\n",
    "X_test = [[col[1], col[2], col[3]] for col in test_data]  # income, age, loan\n",
    "y_test = [col[4] for col in test_data]  # class (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Min-Max Scaling\n",
    "\n",
    "Inputs can have a huge variance amongst themselves. For example `loan` column can have a number ranging from hundreds to millions while the `age` column will most likely stay under a hundred with few exceptions. \n",
    "\n",
    "This can cause some larger features to overshadow the smaller ones. To prevent that, we normalize the data using Min-Max scaling. This technique adjusts each feature so that its values range between 0 and 1. By doing this, we ensure that all features contribute equally to the model's learning process.\n",
    "\n",
    "### $x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write min-max scaling function, instead of using third-party libraries\n",
    "def min_max_scaling(X):\n",
    "    min_vals = [min(col) for col in zip(*X)]\n",
    "    max_vals = [max(col) for col in zip(*X)]\n",
    "\n",
    "    X_scaled = []\n",
    "    for row in X:\n",
    "        scaled_row = [\n",
    "            (row[i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
    "            for i in range(len(row))\n",
    "        ]\n",
    "        X_scaled.append(scaled_row)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "# Normalize test and train data\n",
    "X_train_scaled = min_max_scaling(X_train)\n",
    "X_test_scaled = min_max_scaling(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Weights and Biases\n",
    "In this step, we initialize the weights and biases for our neural network. This is a crucial step as it sets the starting point for the training process.\n",
    "\n",
    "1. **Function to Initialize Random Weights:**\n",
    "   - We define a function `random_matrix(rows, cols)` that generates a matrix of random values with the specified number of rows and columns. This function does not use NumPy and relies on Python's built-in `random` module.\n",
    "\n",
    "2. **Define Network Architecture:**\n",
    "   - `input_size`: The number of input features, which is determined by the length of the first row in `X_train_scaled`. In this case, we have 3 input features: income, age, and loan.\n",
    "   - `hidden_size`: The number of neurons in the hidden layer. We set this to 4.\n",
    "   - `output_size`: The number of output neurons. We set this to 1, as we are predicting a single value.\n",
    "\n",
    "3. **Initialize Weights and Biases:**\n",
    "   - `W1`: A matrix of random weights connecting the input layer to the hidden layer. It has dimensions `input_size x hidden_size`.\n",
    "   - `b1`: A list of random biases for the hidden layer. It has a length of `hidden_size`.\n",
    "   - `W2`: A matrix of random weights connecting the hidden layer to the output layer. It has dimensions `hidden_size x output_size`.\n",
    "   - `b2`: A list of random biases for the output layer. It has a length of `output_size`.\n",
    "\n",
    "By initializing the weights and biases randomly, we ensure that the neural network starts with a diverse set of parameters, which helps in breaking symmetry and allows the network to learn effectively during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize random weights without using numpy\n",
    "def random_matrix(rows, cols):\n",
    "    return [[random.random() for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = len(X_train_scaled[0])  # 3 input features: income, age, loan\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = random_matrix(input_size, hidden_size)\n",
    "b1 = [random.random() for _ in range(hidden_size)]\n",
    "W2 = random_matrix(hidden_size, output_size)\n",
    "b2 = [random.random() for _ in range(output_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Matrix Multiplication | Dot Product\n",
    "\n",
    "Although in a real project we would use a third-party library to perform matrix multiplication, for learning purposes, we will implement it using just Python.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "- **A**: A matrix with dimensions $m \\times n$.\n",
    "- **B**: A matrix with dimensions $n \\times p$.\n",
    "\n",
    "### Output\n",
    "\n",
    "- A matrix with dimensions $m \\times p$, where each element is the dot product of the corresponding row from **A** and column from **B**.\n",
    "\n",
    "### Matrix Multiplication Equation\n",
    "\n",
    "The element at position $(i, j)$ in the resulting matrix is calculated as:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $A_{ik}$ is the element from the $i$-th row and $k$-th column of matrix **A**.\n",
    "- $B_{kj}$ is the element from the $k$-th row and $j$-th column of matrix **B**.\n",
    "- $C_{ij}$ is the element at the $i$-th row and $j$-th column of the resulting matrix.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "For more information on matrix multiplication, you can refer to the [Wikipedia article on Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication function (for dot product)\n",
    "def matrix_multiply(A, B):\n",
    "    return [\n",
    "        [sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adding Bias to a Matrix\n",
    "\n",
    "In this step, we define a function `add_bias(matrix, bias)` that adds a bias vector to each row of a given matrix.\n",
    "\n",
    "### Where Bias is Coming From:\n",
    "In the context of neural networks, biases are additional parameters that are added to the weighted sum of inputs before applying the activation function. They help the model to fit the data better by providing an additional degree of freedom. \n",
    "\n",
    "### Initialization of Biases:\n",
    "Biases are typically initialized randomly or set to zero at the beginning of the training process. In our neural network, biases are initialized as follows:\n",
    "\n",
    "- **`b1`**: A list of random biases for the hidden layer. It has a length equal to the number of neurons in the hidden layer.\n",
    "- **`b2`**: A list of random biases for the output layer. It has a length equal to the number of neurons in the output layer.\n",
    "\n",
    "These biases are then added to the respective layers during the forward pass of the neural network.\n",
    "\n",
    "### Biases During Training:\n",
    "- **Forward Pass**: During the forward pass, biases are added to the weighted sum of inputs but do not change.\n",
    "\n",
    "- **Backward Pass**: During backpropagation, biases are updated along with weights to minimize the error between the predicted output and the actual output. This adjustment is done using optimization algorithms like gradient descent.\n",
    "\n",
    "### Detailed Explanation:\n",
    "1. **Error Calculation**: During the backward pass, the error (or loss) is calculated as the difference between the predicted output of the neural network and the actual output (ground truth). Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "2. **Gradient Calculation**: The gradients of the loss function with respect to each weight and bias are computed. These gradients indicate the direction and magnitude of change needed to reduce the error. This process involves applying the chain rule of calculus to propagate the error backward through the network layers.\n",
    "\n",
    "3. **Parameter Update**: Using the calculated gradients, the weights and biases are updated to minimize the error. This is typically done using an optimization algorithm like gradient descent. In gradient descent, each parameter (weight or bias) is adjusted in the opposite direction of its gradient by a small step, known as the learning rate.\n",
    "\n",
    "4. **Iterative Process**: The process of forward pass, error calculation, gradient calculation, and parameter update is repeated iteratively for many epochs (complete passes through the training dataset) until the model converges to a solution with minimal error.\n",
    "\n",
    "By updating the biases (and weights) during backpropagation, the neural network learns to make more accurate predictions, effectively minimizing the error over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias to a matrix\n",
    "def add_bias(matrix, bias):\n",
    "    return [\n",
    "        [matrix[row][col] + bias[col] for col in range(len(bias))]\n",
    "        for row in range(len(matrix))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sigmoid Function & Forward Propagation\n",
    "The `sigmoid` function is an activation function used in neural networks to introduce non-linearity. It maps any input Z (the weighted sum of inputs to a neuron) to a value between 0 and 1. The function's output can be interpreted as the probability of belonging to class 1. Values close to 0 represent class 0, while values close to 1 represent class 1, creating a clear decision boundary.\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "### $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "The `apply_sigmoid` function applies the sigmoid function to each element of a given matrix. It processes the matrix element-wise, returning a new matrix with the sigmoid function applied to each element.\n",
    "\n",
    "The `forward` function performs the forward propagation through the neural network. Forward propagation involves calculating the activations of each layer in the network, starting from the input layer and moving through to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "# Element-wise application of the sigmoid function\n",
    "def apply_sigmoid(matrix):\n",
    "    return [[sigmoid(x) for x in row] for row in matrix]\n",
    "\n",
    "# Forward propagation\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = add_bias(matrix_multiply(X, W1), b1)  # Input to hidden layer\n",
    "    A1 = apply_sigmoid(Z1)  # Activation in hidden layer\n",
    "    Z2 = add_bias(matrix_multiply(A1, W2), b2)  # Input to output layer\n",
    "    A2 = apply_sigmoid(Z2)  # Final output (prediction)\n",
    "    return A1, A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sigmoid Derivative\n",
    "\n",
    "The derivative of the sigmoid function is used calculating the gradient for adjusting the weights during training.\n",
    "\n",
    "This derivative indicates how the output of the sigmoid function changes with respect to the input, which is essential for optimizing the model.\n",
    "\n",
    "The formula for the sigmoid derivative function is:\n",
    "### $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Backpropagation\n",
    "\n",
    "Backpropagation calculates the error and updates the model's weights and biases to minimize this error.\n",
    "\n",
    "### Helper Functions\n",
    "- **`compute_layer_gradients(dZ, A, m)`**: Calculates gradients for weights and biases for any layer.\n",
    "- **`update_weights_biases(W, b, dW, db, learning_rate)`**: Applies gradient descent to update weights and biases.\n",
    "\n",
    "### Main Function: `backprop(...)`\n",
    "- **Inputs**: Input data (`X`), labels (`y`), weights (`W1`, `W2`), biases (`b1`, `b2`), activations (`A1`, `A2`), and learning rate.\n",
    "- **Process**:\n",
    "  1. Computes error at the output.\n",
    "  2. Calculates and backpropagates gradients.\n",
    "  3. Updates weights and biases for both layers.\n",
    "- **Output**: Updated weights and biases for further training iterations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_gradients(dZ, A, m):\n",
    "    \"\"\"Compute the gradients for weights and biases in a layer.\"\"\"\n",
    "    dW = [[sum(A[i][j] * dZ[i][0] for i in range(m)) / m for _ in range(len(dZ[0]))] for j in range(len(A[0]))]\n",
    "    db = [sum(dZ[i][0] for i in range(m)) / m]\n",
    "    return dW, db\n",
    "\n",
    "def update_weights_biases(W, b, dW, db, learning_rate):\n",
    "    \"\"\"Update weights and biases using gradient descent.\"\"\"\n",
    "    W_updated = [[W[j][i] - learning_rate * dW[j][i] for i in range(len(W[0]))] for j in range(len(W))]\n",
    "    b_updated = [b[i] - learning_rate * db[i] for i in range(len(b))]\n",
    "    return W_updated, b_updated\n",
    "\n",
    "def backprop(X, y, W1, b1, W2, b2, A1, A2, learning_rate=0.1):\n",
    "    m = len(y)  # Number of training examples\n",
    "\n",
    "    # Step 1: Compute the error at the output layer\n",
    "    dZ2 = [[A2[i][0] - y[i]] for i in range(m)]\n",
    "\n",
    "    # Step 2: Calculate gradients at the output layer\n",
    "    dW2, db2 = compute_layer_gradients(dZ2, A1, m)\n",
    "\n",
    "    # Step 3: Propagate the error back to the hidden layer\n",
    "    dA1 = [[sum(W2[h][o] * dZ2[i][0] for o in range(len(W2[0]))) for h in range(len(W2))] for i in range(m)]\n",
    "\n",
    "    # Step 4: Compute the error term at the hidden layer\n",
    "    dZ1 = [[dA1[i][h] * sigmoid_derivative(A1[i][h]) for h in range(len(A1[0]))] for i in range(m)]\n",
    "\n",
    "    # Step 5: Calculate gradients at the hidden layer\n",
    "    dW1, db1 = compute_layer_gradients(dZ1, X, m)\n",
    "\n",
    "    # Step 6: Update weights and biases using gradient descent\n",
    "    W1, b1 = update_weights_biases(W1, b1, dW1, db1, learning_rate)\n",
    "    W2, b2 = update_weights_biases(W2, b2, dW2, db2, learning_rate)\n",
    "\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Training Loop\n",
    "\n",
    "The training loop iterates over a specified number of epochs to train the neural network. During each epoch, the forward and backward propagation steps are performed, and optionally, the loss is calculated for monitoring purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3317760965068706\n",
      "Epoch 1000, Loss: 0.39258092513604625\n",
      "Epoch 2000, Loss: 0.3791446938451353\n",
      "Epoch 3000, Loss: 0.34456667813829983\n",
      "Epoch 4000, Loss: 0.29808416957613454\n",
      "Epoch 5000, Loss: 0.25433260579048994\n",
      "Epoch 6000, Loss: 0.21775096391199814\n",
      "Epoch 7000, Loss: 0.19001783867416813\n",
      "Epoch 8000, Loss: 0.1700869721003493\n",
      "Epoch 9000, Loss: 0.15602323353907965\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10000):  # Number of epochs\n",
    "    A1, A2 = forward(X_train_scaled, W1, b1, W2, b2)\n",
    "    W1, b1, W2, b2 = backprop(X_train_scaled, y_train, W1, b1, W2, b2, A1, A2)\n",
    "\n",
    "    # Optional: Calculate loss for monitoring\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = sum(\n",
    "            -y_train[i] * math.log(A2[i][0]) - (1 - y_train[i]) * math.log(1 - A2[i][0])\n",
    "            for i in range(len(y_train))\n",
    "        ) / len(y_train)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Saving Weights and Biases\n",
    "\n",
    "This code snippet defines a function to save the weights and biases of a neural network to CSV files after training. This allows for the persistence of the model's parameters, which can be reloaded later for inference or further training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights and biases after training\n",
    "def save_weights_biases(W, b, W_file, b_file):\n",
    "    with open(W_file, \"w\") as f_w, open(b_file, \"w\") as f_b:\n",
    "        for row in W:\n",
    "            f_w.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "        f_b.write(\",\".join(map(str, b)) + \"\\n\")\n",
    "\n",
    "\n",
    "save_weights_biases(W1, b1, \"model_weights/W1.csv\", \"model_weights/b1.csv\")\n",
    "save_weights_biases(W2, b2, \"model_weights/W2.csv\", \"model_weights/b2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Testing the model\n",
    "\n",
    "* Forward Propagation on Test Data:The forward function is called with the test data (X_test_scaled), weights (W1, W2), and biases (b1, b2). It returns the activations of the hidden layer (A1_test) and the output layer (A2_test).\n",
    "\n",
    "\n",
    "* Generating Predictions:Predictions are generated by applying a threshold of 0.5 to the output activations (A2_test). If the activation is greater than 0.5, the prediction is 1; otherwise, it is 0.\n",
    "\n",
    "\n",
    "* Calculating Accuracy:The accuracy is calculated by comparing the predictions with the true labels (y_test). The number of correct predictions is summed and divided by the total number of test samples to get the accuracy.\n",
    "Printing Accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "A1_test, A2_test = forward(X_test_scaled, W1, b1, W2, b2)\n",
    "predictions = [1 if a > 0.5 else 0 for a in [row[0] for row in A2_test]]\n",
    "accuracy = sum([1 for i in range(len(y_test)) if predictions[i] == y_test[i]]) / len(\n",
    "    y_test\n",
    ")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-projects)",
   "language": "python",
   "name": "ml-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
