{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "// TODO: Write a introduction part here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "In this step, we import the project dependencies and data. Typically, we would also perform data cleaning and wrangling here, but since our data is already clean and well-structured, we only ensure that the data types are read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Read CSV file without checking for missing values\n",
    "data = []\n",
    "with open(\"data/full_data.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    headers = next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        # Convert relevant columns to numerical values\n",
    "        income = float(row[1])\n",
    "        age = float(row[2])\n",
    "        loan = float(row[3])\n",
    "        target = int(row[4])\n",
    "        # Append the row to the data\n",
    "        data.append([row[0], income, age, loan, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split Data\n",
    "\n",
    "In this step, we will split the data into two parts: training data and test data. The training data will be used to teach the model, while the test data will help us check how well the model works on new, unseen data. This way, we can ensure that the model is not just memorizing the training data but is actually learning patterns that can be applied in real situations.\n",
    "\n",
    "A model can \"memorize\" the data if it’s too complex or if it’s trained for too long on a small dataset. This is called overfitting. When overfitting happens, the model performs very well on the training data but struggles with new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(data)\n",
    "\n",
    "# Define the split ratio (80% training, 20% testing)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "\n",
    "# Save the training data to a CSV file\n",
    "with open(\"data/train_data.csv\", \"w\", newline=\"\") as trainfile:\n",
    "    writer = csv.writer(trainfile)\n",
    "    writer.writerow(headers)  # Write the header\n",
    "    writer.writerows(train_data)  # Write the training data\n",
    "\n",
    "\n",
    "# Save the testing data to a CSV file\n",
    "with open(\"data/test_data.csv\", \"w\", newline=\"\") as testfile:\n",
    "    writer = csv.writer(testfile)\n",
    "    writer.writerow(headers)  # Write the header\n",
    "    writer.writerows(test_data)  # Write the testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction and Target Variable Selection\n",
    "\n",
    "In this step, we extract the relevant columns from the dataset. The columns `income`, `age`, and `loan` are selected as features, which are stored in `X_train` and `X_test`. These features will be used as input to train and evaluate the model.\n",
    "\n",
    "Additionally, we select the `class` column as the target variable, which is stored in `y_train` and `y_test`. The target variable represents the outcome or category we want the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target from training data\n",
    "X_train = [[row[1], row[2], row[3]] for row in train_data]  # income, age, loan\n",
    "y_train = [row[4] for row in train_data]  # class (target)\n",
    "\n",
    "# Extract features and target from testing data\n",
    "X_test = [[row[1], row[2], row[3]] for row in test_data]  # income, age, loan\n",
    "y_test = [row[4] for row in test_data]  # class (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Min-Max Scaling\n",
    "\n",
    "In this step, we will normalize the features of our dataset using Min-Max scaling. This technique adjusts each feature so that its values range between 0 and 1. By doing this, we ensure that all features contribute equally to the model's learning process, preventing features with larger value ranges from overshadowing those with smaller ranges.\n",
    "\n",
    "\n",
    "The formula for the min-max function is:\n",
    "### $x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually normalize the data using Min-Max scaling\n",
    "def min_max_scaling(X):\n",
    "    min_vals = [min(col) for col in zip(*X)]\n",
    "    max_vals = [max(col) for col in zip(*X)]\n",
    "\n",
    "    X_scaled = []\n",
    "    for row in X:\n",
    "        scaled_row = [\n",
    "            (row[i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
    "            for i in range(len(row))\n",
    "        ]\n",
    "        X_scaled.append(scaled_row)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "X_train_scaled = min_max_scaling(X_train)\n",
    "X_test_scaled = min_max_scaling(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sigmoid & Sigmoid Derivative\n",
    "The sigmoid function is an activation function used in neural networks to introduce non-linearity. It maps any input Z (the weighted sum of inputs to a neuron) to a value between 0 and 1. The function's output can be interpreted as the probability of belonging to class 1. Values close to 0 represent class 0, while values close to 1 represent class 1, creating a clear decision boundary.\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "### $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "The derivative of the sigmoid function is important for backpropagation in neural networks, as it helps calculate the gradient for adjusting the weights during training:\n",
    "### $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "This derivative shows how the output of the sigmoid changes with respect to the input, which is essential for optimizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Weights and Biases\n",
    "In this step, we initialize the weights and biases for our neural network. This is a crucial step as it sets the starting point for the training process.\n",
    "\n",
    "1. **Function to Initialize Random Weights:**\n",
    "   - We define a function `random_matrix(rows, cols)` that generates a matrix of random values with the specified number of rows and columns. This function does not use NumPy and relies on Python's built-in `random` module.\n",
    "\n",
    "2. **Define Network Architecture:**\n",
    "   - `input_size`: The number of input features, which is determined by the length of the first row in `X_train_scaled`. In this case, we have 3 input features: income, age, and loan.\n",
    "   - `hidden_size`: The number of neurons in the hidden layer. We set this to 4.\n",
    "   - `output_size`: The number of output neurons. We set this to 1, as we are predicting a single value.\n",
    "\n",
    "3. **Initialize Weights and Biases:**\n",
    "   - `W1`: A matrix of random weights connecting the input layer to the hidden layer. It has dimensions `input_size x hidden_size`.\n",
    "   - `b1`: A list of random biases for the hidden layer. It has a length of `hidden_size`.\n",
    "   - `W2`: A matrix of random weights connecting the hidden layer to the output layer. It has dimensions `hidden_size x output_size`.\n",
    "   - `b2`: A list of random biases for the output layer. It has a length of `output_size`.\n",
    "\n",
    "By initializing the weights and biases randomly, we ensure that the neural network starts with a diverse set of parameters, which helps in breaking symmetry and allows the network to learn effectively during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize random weights without using numpy\n",
    "def random_matrix(rows, cols):\n",
    "    return [[random.random() for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = len(X_train_scaled[0])  # 3 input features: income, age, loan\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = random_matrix(input_size, hidden_size)\n",
    "b1 = [random.random() for _ in range(hidden_size)]\n",
    "W2 = random_matrix(hidden_size, output_size)\n",
    "b2 = [random.random() for _ in range(output_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication function (for dot product)\n",
    "def matrix_multiply(A, B):\n",
    "    return [\n",
    "        [sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias to a matrix\n",
    "def add_bias(matrix, bias):\n",
    "    return [\n",
    "        [matrix[row][col] + bias[col] for col in range(len(bias))]\n",
    "        for row in range(len(matrix))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise application of the sigmoid function\n",
    "def apply_sigmoid(matrix):\n",
    "    return [[sigmoid(x) for x in row] for row in matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = add_bias(matrix_multiply(X, W1), b1)  # Input to hidden layer\n",
    "    A1 = apply_sigmoid(Z1)  # Activation in hidden layer\n",
    "    Z2 = add_bias(matrix_multiply(A1, W2), b2)  # Input to output layer\n",
    "    A2 = apply_sigmoid(Z2)  # Final output (prediction)\n",
    "    return A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (for one epoch)\n",
    "def backprop(X, y, W1, b1, W2, b2, A1, A2, learning_rate=0.1):\n",
    "    m = len(y)\n",
    "\n",
    "    dZ2 = [[A2[i][0] - y[i]] for i in range(m)]  # Derivative of loss w.r.t output\n",
    "    dW2 = [\n",
    "        [sum(A1[i][h] * dZ2[i][0] for i in range(m)) / m for _ in range(output_size)]\n",
    "        for h in range(hidden_size)\n",
    "    ]\n",
    "    db2 = [sum(dZ2[i][0] for i in range(m)) / m]\n",
    "\n",
    "    dA1 = [\n",
    "        [\n",
    "            sum(W2[h][o] * dZ2[i][0] for o in range(output_size))\n",
    "            for h in range(hidden_size)\n",
    "        ]\n",
    "        for i in range(m)\n",
    "    ]\n",
    "    dZ1 = [\n",
    "        [dA1[i][h] * sigmoid_derivative(A1[i][h]) for h in range(hidden_size)]\n",
    "        for i in range(m)\n",
    "    ]\n",
    "    dW1 = [\n",
    "        [sum(X[i][f] * dZ1[i][h] for i in range(m)) / m for h in range(hidden_size)]\n",
    "        for f in range(input_size)\n",
    "    ]\n",
    "    db1 = [sum(dZ1[i][h] for i in range(m)) / m for h in range(hidden_size)]\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    W1 = [\n",
    "        [W1[f][h] - learning_rate * dW1[f][h] for h in range(hidden_size)]\n",
    "        for f in range(input_size)\n",
    "    ]\n",
    "    b1 = [b1[h] - learning_rate * db1[h] for h in range(hidden_size)]\n",
    "    W2 = [\n",
    "        [W2[h][o] - learning_rate * dW2[h][o] for o in range(output_size)]\n",
    "        for h in range(hidden_size)\n",
    "    ]\n",
    "    b2 = [b2[o] - learning_rate * db2[o] for o in range(output_size)]\n",
    "\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3317760965068706\n",
      "Epoch 1000, Loss: 0.39258092513604625\n",
      "Epoch 2000, Loss: 0.3791446938451353\n",
      "Epoch 3000, Loss: 0.34456667813829983\n",
      "Epoch 4000, Loss: 0.29808416957613454\n",
      "Epoch 5000, Loss: 0.25433260579048994\n",
      "Epoch 6000, Loss: 0.21775096391199814\n",
      "Epoch 7000, Loss: 0.19001783867416813\n",
      "Epoch 8000, Loss: 0.1700869721003493\n",
      "Epoch 9000, Loss: 0.15602323353907965\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10000):  # Number of epochs\n",
    "    A1, A2 = forward(X_train_scaled, W1, b1, W2, b2)\n",
    "    W1, b1, W2, b2 = backprop(X_train_scaled, y_train, W1, b1, W2, b2, A1, A2)\n",
    "\n",
    "    # Optional: Calculate loss for monitoring\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = sum(\n",
    "            -y_train[i] * math.log(A2[i][0]) - (1 - y_train[i]) * math.log(1 - A2[i][0])\n",
    "            for i in range(len(y_train))\n",
    "        ) / len(y_train)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights and biases after training\n",
    "def save_weights_biases(W, b, W_file, b_file):\n",
    "    with open(W_file, \"w\") as f_w, open(b_file, \"w\") as f_b:\n",
    "        for row in W:\n",
    "            f_w.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "        f_b.write(\",\".join(map(str, b)) + \"\\n\")\n",
    "\n",
    "\n",
    "save_weights_biases(W1, b1, \"model_weights/W1.csv\", \"model_weights/b1.csv\")\n",
    "save_weights_biases(W2, b2, \"model_weights/W2.csv\", \"model_weights/b2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "A1_test, A2_test = forward(X_test_scaled, W1, b1, W2, b2)\n",
    "predictions = [1 if a > 0.5 else 0 for a in [row[0] for row in A2_test]]\n",
    "accuracy = sum([1 for i in range(len(y_test)) if predictions[i] == y_test[i]]) / len(\n",
    "    y_test\n",
    ")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-projects)",
   "language": "python",
   "name": "ml-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
