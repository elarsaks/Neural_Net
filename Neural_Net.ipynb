{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "// TODO: Write a introduction part here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "To get started with any ML project, at first we need data, so lets import it. Typically, we would also perform data cleaning and wrangling here, but since our data is already clean and well-structured, we only ensure that the data types are read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Read CSV file without checking for missing values\n",
    "data = []\n",
    "with open(\"data/full_data.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    headers = next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        # Convert relevant columns to numerical values\n",
    "        income = float(row[1])\n",
    "        age = float(row[2])\n",
    "        loan = float(row[3])\n",
    "        target = int(row[4])\n",
    "        # Append the row to the data\n",
    "        data.append([row[0], income, age, loan, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Data into Test & Training Sets\n",
    "\n",
    "In this step, we will split the data into two parts: training and test data. The training data will be used to teach the model, while the test data will help us check how well the model works on new, unseen data. This way, we can ensure that the model is not just memorizing the training data but is actually learning patterns that can be applied in real situations.\n",
    "\n",
    "A model can \"memorize\" the data if trained for too long on a small dataset. This is called overfitting. When overfitting happens, the model performs very well on the training data but struggles with new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(data)\n",
    "\n",
    "# Define the split ratio (80% training, 20% testing)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "\n",
    "# Save the training data to a CSV file\n",
    "with open(\"data/train_data.csv\", \"w\", newline=\"\") as trainfile:\n",
    "    writer = csv.writer(trainfile)\n",
    "    writer.writerow(headers)  # Write the header\n",
    "    writer.writerows(train_data)  # Write the training data\n",
    "\n",
    "\n",
    "# Save the testing data to a CSV file\n",
    "with open(\"data/test_data.csv\", \"w\", newline=\"\") as testfile:\n",
    "    writer = csv.writer(testfile)\n",
    "    writer.writerow(headers)  # Write the header\n",
    "    writer.writerows(test_data)  # Write the testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction and Target Variable Selection\n",
    "\n",
    "Now, that we have divided our data into `test` and `training` sets, lets pick the columns (`income`, `age`, and `loan`) based on what we want to predict the outcome or a last column (`class`).\n",
    "\n",
    "The columns `income`, `age`, and `loan` are selected as features, which are stored in `X_train` and `X_test`. These features will be used as input to train and evaluate the model.\n",
    "\n",
    "Additionally, we select the last column (`class`) as the target variable, which is stored in `y_train` and `y_test`. The target variable represents the outcome or category we want the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target from training data\n",
    "X_train = [[col[1], col[2], col[3]] for col in train_data]  # income, age, loan\n",
    "y_train = [col[4] for col in train_data]  # class (target)\n",
    "\n",
    "# Extract features and target from testing data\n",
    "X_test = [[col[1], col[2], col[3]] for col in test_data]  # income, age, loan\n",
    "y_test = [col[4] for col in test_data]  # class (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Min-Max Scaling\n",
    "\n",
    "Inputs can have a huge variance amongst themselves. For example `loan` column can have a number ranging from hundreds to millions while the `age` column will most likely stay under a hundred with few exceptions. \n",
    "\n",
    "This can cause some larger features to overshadow the smaller ones. To prevent that, we normalize the data using Min-Max scaling. This technique adjusts each feature so that its values range between 0 and 1. By doing this, we ensure that all features contribute equally to the model's learning process.\n",
    "\n",
    "### $x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually normalize the data using Min-Max scaling\n",
    "def min_max_scaling(X):\n",
    "    min_vals = [min(col) for col in zip(*X)]\n",
    "    max_vals = [max(col) for col in zip(*X)]\n",
    "\n",
    "    X_scaled = []\n",
    "    for row in X:\n",
    "        scaled_row = [\n",
    "            (row[i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
    "            for i in range(len(row))\n",
    "        ]\n",
    "        X_scaled.append(scaled_row)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "X_train_scaled = min_max_scaling(X_train)\n",
    "X_test_scaled = min_max_scaling(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Weights and Biases\n",
    "In this step, we initialize the weights and biases for our neural network. This is a crucial step as it sets the starting point for the training process.\n",
    "\n",
    "1. **Function to Initialize Random Weights:**\n",
    "   - We define a function `random_matrix(rows, cols)` that generates a matrix of random values with the specified number of rows and columns. This function does not use NumPy and relies on Python's built-in `random` module.\n",
    "\n",
    "2. **Define Network Architecture:**\n",
    "   - `input_size`: The number of input features, which is determined by the length of the first row in `X_train_scaled`. In this case, we have 3 input features: income, age, and loan.\n",
    "   - `hidden_size`: The number of neurons in the hidden layer. We set this to 4.\n",
    "   - `output_size`: The number of output neurons. We set this to 1, as we are predicting a single value.\n",
    "\n",
    "3. **Initialize Weights and Biases:**\n",
    "   - `W1`: A matrix of random weights connecting the input layer to the hidden layer. It has dimensions `input_size x hidden_size`.\n",
    "   - `b1`: A list of random biases for the hidden layer. It has a length of `hidden_size`.\n",
    "   - `W2`: A matrix of random weights connecting the hidden layer to the output layer. It has dimensions `hidden_size x output_size`.\n",
    "   - `b2`: A list of random biases for the output layer. It has a length of `output_size`.\n",
    "\n",
    "By initializing the weights and biases randomly, we ensure that the neural network starts with a diverse set of parameters, which helps in breaking symmetry and allows the network to learn effectively during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize random weights without using numpy\n",
    "def random_matrix(rows, cols):\n",
    "    return [[random.random() for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = len(X_train_scaled[0])  # 3 input features: income, age, loan\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = random_matrix(input_size, hidden_size)\n",
    "b1 = [random.random() for _ in range(hidden_size)]\n",
    "W2 = random_matrix(hidden_size, output_size)\n",
    "b2 = [random.random() for _ in range(output_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Matrix Multiplication | Dot Product\n",
    "\n",
    "Although in a real project we would use a third-party library to perform matrix multiplication, for learning purposes, we will implement it using just Python.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "- **A**: A matrix with dimensions $m \\times n$.\n",
    "- **B**: A matrix with dimensions $n \\times p$.\n",
    "\n",
    "### Output\n",
    "\n",
    "- A matrix with dimensions $m \\times p$, where each element is the dot product of the corresponding row from **A** and column from **B**.\n",
    "\n",
    "### Matrix Multiplication Equation\n",
    "\n",
    "The element at position $(i, j)$ in the resulting matrix is calculated as:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $A_{ik}$ is the element from the $i$-th row and $k$-th column of matrix **A**.\n",
    "- $B_{kj}$ is the element from the $k$-th row and $j$-th column of matrix **B**.\n",
    "- $C_{ij}$ is the element at the $i$-th row and $j$-th column of the resulting matrix.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "For more information on matrix multiplication, you can refer to the [Wikipedia article on Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication function (for dot product)\n",
    "def matrix_multiply(A, B):\n",
    "    return [\n",
    "        [sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adding Bias to a Matrix\n",
    "\n",
    "In this step, we define a function `add_bias(matrix, bias)` that adds a bias vector to each row of a given matrix.\n",
    "\n",
    "### Where Bias is Coming From:\n",
    "In the context of neural networks, biases are additional parameters that are added to the weighted sum of inputs before applying the activation function. They help the model to fit the data better by providing an additional degree of freedom. \n",
    "\n",
    "### Initialization of Biases:\n",
    "Biases are typically initialized randomly or set to zero at the beginning of the training process. In our neural network, biases are initialized as follows:\n",
    "\n",
    "- **`b1`**: A list of random biases for the hidden layer. It has a length equal to the number of neurons in the hidden layer.\n",
    "- **`b2`**: A list of random biases for the output layer. It has a length equal to the number of neurons in the output layer.\n",
    "\n",
    "These biases are then added to the respective layers during the forward pass of the neural network.\n",
    "\n",
    "### Biases During Training:\n",
    "- **Forward Pass**: During the forward pass, biases are added to the weighted sum of inputs but do not change.\n",
    "\n",
    "- **Backward Pass**: During backpropagation, biases are updated along with weights to minimize the error between the predicted output and the actual output. This adjustment is done using optimization algorithms like gradient descent.\n",
    "\n",
    "### Detailed Explanation:\n",
    "1. **Error Calculation**: During the backward pass, the error (or loss) is calculated as the difference between the predicted output of the neural network and the actual output (ground truth). Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "2. **Gradient Calculation**: The gradients of the loss function with respect to each weight and bias are computed. These gradients indicate the direction and magnitude of change needed to reduce the error. This process involves applying the chain rule of calculus to propagate the error backward through the network layers.\n",
    "\n",
    "3. **Parameter Update**: Using the calculated gradients, the weights and biases are updated to minimize the error. This is typically done using an optimization algorithm like gradient descent. In gradient descent, each parameter (weight or bias) is adjusted in the opposite direction of its gradient by a small step, known as the learning rate.\n",
    "\n",
    "4. **Iterative Process**: The process of forward pass, error calculation, gradient calculation, and parameter update is repeated iteratively for many epochs (complete passes through the training dataset) until the model converges to a solution with minimal error.\n",
    "\n",
    "By updating the biases (and weights) during backpropagation, the neural network learns to make more accurate predictions, effectively minimizing the error over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias to a matrix\n",
    "def add_bias(matrix, bias):\n",
    "    return [\n",
    "        [matrix[row][col] + bias[col] for col in range(len(bias))]\n",
    "        for row in range(len(matrix))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sigmoid Function & Forward Propagation\n",
    "The `sigmoid` function is an activation function used in neural networks to introduce non-linearity. It maps any input Z (the weighted sum of inputs to a neuron) to a value between 0 and 1. The function's output can be interpreted as the probability of belonging to class 1. Values close to 0 represent class 0, while values close to 1 represent class 1, creating a clear decision boundary.\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "### $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "The `apply_sigmoid` function applies the sigmoid function to each element of a given matrix. It processes the matrix element-wise, returning a new matrix with the sigmoid function applied to each element.\n",
    "\n",
    "The `forward` function performs the forward propagation through the neural network. Forward propagation involves calculating the activations of each layer in the network, starting from the input layer and moving through to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "# Element-wise application of the sigmoid function\n",
    "def apply_sigmoid(matrix):\n",
    "    return [[sigmoid(x) for x in row] for row in matrix]\n",
    "\n",
    "# Forward propagation\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = add_bias(matrix_multiply(X, W1), b1)  # Input to hidden layer\n",
    "    A1 = apply_sigmoid(Z1)  # Activation in hidden layer\n",
    "    Z2 = add_bias(matrix_multiply(A1, W2), b2)  # Input to output layer\n",
    "    A2 = apply_sigmoid(Z2)  # Final output (prediction)\n",
    "    return A1, A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sigmoid Derivative\n",
    "\n",
    "The derivative of the sigmoid function is used calculating the gradient for adjusting the weights during training.\n",
    "\n",
    "This derivative indicates how the output of the sigmoid function changes with respect to the input, which is essential for optimizing the model.\n",
    "\n",
    "The formula for the sigmoid derivative function is:\n",
    "### $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Backpropagation\n",
    "\n",
    "In this step, we define the backpropagation process, which calculates the gradients of the loss function with respect to each weight and bias, and updates them to minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, y, W1, b1, W2, b2, A1, A2, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Perform backpropagation to update the weights and biases of a simple neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data, a list of input vectors (size m x input_size).\n",
    "    - y: True labels, a list of outputs (size m).\n",
    "    - W1, b1: Weights and biases of the first (hidden) layer.\n",
    "    - W2, b2: Weights and biases of the second (output) layer.\n",
    "    - A1: Activations from the hidden layer (size m x hidden_size).\n",
    "    - A2: Activations from the output layer (size m x output_size).\n",
    "    - learning_rate: Learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    - Updated weights and biases: W1, b1, W2, b2.\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(y)  # Number of training examples\n",
    "\n",
    "    # --- Step 1: Compute the error at the output layer ---\n",
    "    # dZ2 represents the derivative of the loss with respect to the output activation A2\n",
    "    # It's the difference between the predicted output (A2) and the true labels (y)\n",
    "    dZ2 = [[A2[i][0] - y[i]] for i in range(m)]  # Size: m x 1\n",
    "\n",
    "    # --- Step 2: Calculate gradients at the output layer ---\n",
    "    # dW2 is the gradient of the loss with respect to W2 (weights between hidden and output layers)\n",
    "    # It's calculated as the dot product between the activations of the hidden layer (A1) \n",
    "    # and the error term from the output layer (dZ2), averaged over all examples.\n",
    "    dW2 = [\n",
    "        [\n",
    "            sum(A1[i][h] * dZ2[i][0] for i in range(m)) / m  # Average over all examples\n",
    "            for _ in range(output_size)\n",
    "        ]\n",
    "        for h in range(hidden_size)\n",
    "    ]  # Size: hidden_size x output_size\n",
    "\n",
    "    # db2 is the gradient of the loss with respect to b2 (biases of the output layer)\n",
    "    # It's the average of the output layer errors (dZ2) over all examples.\n",
    "    db2 = [sum(dZ2[i][0] for i in range(m)) / m]  # Size: output_size\n",
    "\n",
    "    # --- Step 3: Propagate the error back to the hidden layer ---\n",
    "    # Compute dA1, the derivative of the loss with respect to the activations A1 of the hidden layer.\n",
    "    # This is done by multiplying the errors at the output layer (dZ2) with the weights connecting the\n",
    "    # hidden layer to the output layer (W2), for each example.\n",
    "    dA1 = [\n",
    "        [\n",
    "            sum(W2[h][o] * dZ2[i][0] for o in range(output_size))  # Sum over output neurons\n",
    "            for h in range(hidden_size)\n",
    "        ]\n",
    "        for i in range(m)\n",
    "    ]  # Size: m x hidden_size\n",
    "\n",
    "    # --- Step 4: Compute the error term at the hidden layer ---\n",
    "    # dZ1 represents the derivative of the loss with respect to the pre-activation (Z1) of the hidden layer.\n",
    "    # We calculate this by multiplying the derivative of the activation function (sigmoid_derivative)\n",
    "    # with the error term propagated back to the hidden layer (dA1).\n",
    "    dZ1 = [\n",
    "        [dA1[i][h] * sigmoid_derivative(A1[i][h]) for h in range(hidden_size)]\n",
    "        for i in range(m)\n",
    "    ]  # Size: m x hidden_size\n",
    "\n",
    "    # --- Step 5: Calculate gradients at the hidden layer ---\n",
    "    # dW1 is the gradient of the loss with respect to W1 (weights between input and hidden layers).\n",
    "    # It's calculated as the dot product between the input data (X) and the hidden layer error term (dZ1),\n",
    "    # averaged over all examples.\n",
    "    dW1 = [\n",
    "        [\n",
    "            sum(X[i][f] * dZ1[i][h] for i in range(m)) / m  # Average over all examples\n",
    "            for h in range(hidden_size)\n",
    "        ]\n",
    "        for f in range(input_size)\n",
    "    ]  # Size: input_size x hidden_size\n",
    "\n",
    "    # db1 is the gradient of the loss with respect to b1 (biases of the hidden layer).\n",
    "    # It's the average of the hidden layer errors (dZ1) over all examples.\n",
    "    db1 = [\n",
    "        sum(dZ1[i][h] for i in range(m)) / m  # Average over all examples\n",
    "        for h in range(hidden_size)\n",
    "    ]  # Size: hidden_size\n",
    "\n",
    "    # --- Step 6: Update weights and biases using gradient descent ---\n",
    "    # Update W2 by subtracting the product of the learning rate and dW2\n",
    "    W2 = [[W2[h][o] - learning_rate * dW2[h][o] for o in range(output_size)] for h in range(hidden_size)]\n",
    "    \n",
    "    # Update b2 by subtracting the product of the learning rate and db2\n",
    "    b2 = [b2[o] - learning_rate * db2[o] for o in range(output_size)]\n",
    "    \n",
    "    # Update W1 by subtracting the product of the learning rate and dW1\n",
    "    W1 = [[W1[f][h] - learning_rate * dW1[f][h] for h in range(hidden_size)] for f in range(input_size)]\n",
    "    \n",
    "    # Update b1 by subtracting the product of the learning rate and db1\n",
    "    b1 = [b1[h] - learning_rate * db1[h] for h in range(hidden_size)]\n",
    "\n",
    "    # Return updated weights and biases\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Training Loop\n",
    "\n",
    "The training loop iterates over a specified number of epochs to train the neural network. During each epoch, the forward and backward propagation steps are performed, and optionally, the loss is calculated for monitoring purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3317760965068706\n",
      "Epoch 1000, Loss: 0.39258092513604625\n",
      "Epoch 2000, Loss: 0.3791446938451353\n",
      "Epoch 3000, Loss: 0.34456667813829983\n",
      "Epoch 4000, Loss: 0.29808416957613454\n",
      "Epoch 5000, Loss: 0.25433260579048994\n",
      "Epoch 6000, Loss: 0.21775096391199814\n",
      "Epoch 7000, Loss: 0.19001783867416813\n",
      "Epoch 8000, Loss: 0.1700869721003493\n",
      "Epoch 9000, Loss: 0.15602323353907965\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10000):  # Number of epochs\n",
    "    A1, A2 = forward(X_train_scaled, W1, b1, W2, b2)\n",
    "    W1, b1, W2, b2 = backprop(X_train_scaled, y_train, W1, b1, W2, b2, A1, A2)\n",
    "\n",
    "    # Optional: Calculate loss for monitoring\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = sum(\n",
    "            -y_train[i] * math.log(A2[i][0]) - (1 - y_train[i]) * math.log(1 - A2[i][0])\n",
    "            for i in range(len(y_train))\n",
    "        ) / len(y_train)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights and biases after training\n",
    "def save_weights_biases(W, b, W_file, b_file):\n",
    "    with open(W_file, \"w\") as f_w, open(b_file, \"w\") as f_b:\n",
    "        for row in W:\n",
    "            f_w.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "        f_b.write(\",\".join(map(str, b)) + \"\\n\")\n",
    "\n",
    "\n",
    "save_weights_biases(W1, b1, \"model_weights/W1.csv\", \"model_weights/b1.csv\")\n",
    "save_weights_biases(W2, b2, \"model_weights/W2.csv\", \"model_weights/b2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "A1_test, A2_test = forward(X_test_scaled, W1, b1, W2, b2)\n",
    "predictions = [1 if a > 0.5 else 0 for a in [row[0] for row in A2_test]]\n",
    "accuracy = sum([1 for i in range(len(y_test)) if predictions[i] == y_test[i]]) / len(\n",
    "    y_test\n",
    ")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-projects)",
   "language": "python",
   "name": "ml-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
